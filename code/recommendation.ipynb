{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/clean_dataset.csv\")\n",
    "\n",
    "# fixing random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)  # PyTorch CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.manual_seed(seed)  # Seed for MPS devices\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(\n",
    "    data, test_size=0.1, random_state=seed, stratify=data.rating.values\n",
    ")\n",
    "train, validation = train_test_split(\n",
    "    train, test_size=0.1, random_state=seed, stratify=train.rating.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pytorch tensors\n",
    "train_users = torch.tensor(train[\"user\"].values, dtype=torch.long)\n",
    "train_books = torch.tensor(train[\"book_id\"].values, dtype=torch.long)\n",
    "train_ratings = torch.tensor(train[\"rating\"].values, dtype=torch.float32)\n",
    "\n",
    "val_users = torch.tensor(validation[\"user\"].values, dtype=torch.long)\n",
    "val_books = torch.tensor(validation[\"book_id\"].values, dtype=torch.long)\n",
    "val_ratings = torch.tensor(validation[\"rating\"].values, dtype=torch.float32)\n",
    "\n",
    "test_users = torch.tensor(test[\"user\"].values, dtype=torch.long)\n",
    "test_books = torch.tensor(test[\"book_id\"].values, dtype=torch.long)\n",
    "test_ratings = torch.tensor(test[\"rating\"].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization - SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# build trainset\n",
    "SVD_train_set = Dataset.load_from_df(\n",
    "    train[[\"user\", \"book_id\", \"rating\"]], reader\n",
    ").build_full_trainset()\n",
    "\n",
    "# make validation and test set in the right format: list of tuples\n",
    "SVD_val_set = list(\n",
    "    validation[[\"user\", \"book_id\", \"rating\"]].itertuples(index=False, name=None)\n",
    ")\n",
    "SVD_test_set = list(\n",
    "    test[[\"user\", \"book_id\", \"rating\"]].itertuples(index=False, name=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "\n",
    "algo = SVD(\n",
    "    n_factors=95,\n",
    "    lr_all=0.015,\n",
    "    init_std_dev=0.05,\n",
    "    reg_all=0.015,\n",
    "    n_epochs=300,\n",
    "    random_state=seed,\n",
    ")\n",
    "algo.fit(SVD_train_set)\n",
    "val_predictions = algo.test(SVD_val_set)\n",
    "test_predictions = algo.test(SVD_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set MSE: 0.44198889647470835\n",
      "test set RMSE: 0.6648224548514501\n"
     ]
    }
   ],
   "source": [
    "from surprise.accuracy import rmse\n",
    "from surprise.accuracy import mse\n",
    "\n",
    "print(f\"test set MSE: {mse(test_predictions, verbose=False)}\")\n",
    "print(f\"test set RMSE: {rmse(test_predictions, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import dump\n",
    "\n",
    "dump.dump(\"../models/svd_model.pkl\", algo=algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, user_ids, book_ids, ratings):\n",
    "        self.user_ids = user_ids\n",
    "        self.book_ids = book_ids\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"user_id\": self.user_ids[idx],\n",
    "            \"book_id\": self.book_ids[idx],\n",
    "            \"rating\": self.ratings[idx],\n",
    "        }\n",
    "\n",
    "# instatiate datasets objects\n",
    "train_dataset = Dataset(train_users, train_books, train_ratings)\n",
    "val_dataset = Dataset(val_users, val_books, val_ratings)\n",
    "test_dataset = Dataset(test_users, test_books, test_ratings)\n",
    "\n",
    "# instatiate dataloader objects\n",
    "batch_size = 32 # 64?\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringModel(nn.Module):\n",
    "    def __init__(self, n_users, n_books, hidden_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.book_embedding = nn.Embedding(n_books, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, book_id):\n",
    "        user_embedded = self.user_embedding(user_id)\n",
    "        book_embedded = self.book_embedding(book_id)\n",
    "        x = torch.cat([user_embedded, book_embedded], dim=-1)\n",
    "\n",
    "        return self.fc(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU if MPS is not available\n",
    "    print(\"MPS is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float(\"inf\")\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "n_users = data[\"user\"].nunique()\n",
    "n_books = data[\"title\"].nunique()\n",
    "\n",
    "# hyperparameters\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "epochs = 50\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopper(patience=8, min_delta=0.001)\n",
    "\n",
    "# Initialize the model\n",
    "model = CollaborativeFilteringModel(\n",
    "    n_users=n_users, n_books=n_books, hidden_dim=hidden_dim, embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, device):\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            user_ids = batch[\"user_id\"].to(device)\n",
    "            book_ids = batch[\"book_id\"].to(device)\n",
    "            y_pred.append(model(user_ids, book_ids))\n",
    "            y_test.append(batch[\"rating\"].to(device))\n",
    "\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_test = torch.cat(y_test, dim=0)\n",
    "        \n",
    "    return y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    early_stopper,\n",
    "    device,\n",
    "):\n",
    "    best_valid_loss = float(\"inf\")  # infinite\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            user_ids = batch[\"user_id\"].to(device)\n",
    "            book_ids = batch[\"book_id\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(user_ids, book_ids)\n",
    "\n",
    "            ratings = batch['rating'].to(device)\n",
    "\n",
    "            # compute training loss\n",
    "            loss = criterion(predictions, ratings)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_train_loss = total_loss / len(train_loader)\n",
    "        ratings, preds = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(preds, ratings)\n",
    "        print(\n",
    "            f\"epoch {epoch}: training loss: {average_train_loss}, validation loss: {loss_val}\"\n",
    "        )\n",
    "\n",
    "        # save best model\n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            if not os.path.exists(\"models\"):\n",
    "                os.makedirs(\"models\")\n",
    "            torch.save(model.state_dict(), \"models/collaborative_model.pth\")\n",
    "\n",
    "        # early stopper\n",
    "        if early_stopper.early_stop(loss_val):             \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/fjbkbl1119z07xq7zv89lw240000gn/T/ipykernel_865/1147581042.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"models/collaborative_model.pth\")\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"models/collaborative_model.pth\"):\n",
    "   state_dict = torch.load(\"models/collaborative_model.pth\")\n",
    "   model.load_state_dict(state_dict)\n",
    "else:\n",
    "   train_model(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    early_stopper=early_stopping,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test NCF on the the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE validation set: 0.5652198791503906 \n",
      "RMSE validation set: 0.7518110871315002\n"
     ]
    }
   ],
   "source": [
    "ratings, preds = test_model(model=model, loader=val_loader, device=device)\n",
    "loss_test = criterion(preds, ratings)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = torch.sqrt(loss_test)\n",
    "print(f\"MSE validation set: {loss_test.item()} \\nRMSE validation set: {rmse.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1122)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1122)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1122)>\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "book_train = train.drop_duplicates(subset=\"book_id\", keep=\"first\")\n",
    "book_train = book_train.drop([\"user\", \"rating\"], axis=1)\n",
    "book_train[\"categories\"] = (\n",
    "    book_train[\"categories\"]\n",
    "    .fillna(\"unknown\")\n",
    "    .apply(lambda x: re.sub(r\"[\\[\\]']\", \"\", x))\n",
    ")\n",
    "book_train[\"full_info\"] = book_train[\"categories\"] + \" \" + book_train[\"description\"]\n",
    "book_train[\"processed_text\"] = book_train[\"full_info\"].apply(preprocess_text)\n",
    "\n",
    "train_sentences = book_train[\"processed_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "bigram_transformer = Phrases(train_sentences)\n",
    "\n",
    "bi_train_sentences = [bigram_transformer[sentence] for sentence in train_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 69432048.0\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    vector_size=400,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1,\n",
    "    workers=5,\n",
    "    seed=seed,\n",
    ")\n",
    "word2vec_model.build_vocab(bi_train_sentences)\n",
    "\n",
    "word2vec_model.train(\n",
    "    bi_train_sentences,\n",
    "    total_examples=word2vec_model.corpus_count,\n",
    "    epochs=30,\n",
    "    compute_loss=True,\n",
    "    start_alpha=0.01,\n",
    "    end_alpha=0.001,\n",
    ")\n",
    "\n",
    "print(\"Training Loss:\", word2vec_model.get_latest_training_loss())\n",
    "\n",
    "# Save the fine-tuned model\n",
    "word2vec_model.save(\"../models/local_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "local_model = Word2Vec.load(\"../models/local_word2vec.model\")\n",
    "glove_input_file = \"../models/glove.6B.300d.txt\"\n",
    "pretrained_vectors = KeyedVectors.load_word2vec_format(\n",
    "    glove_input_file, binary=False, no_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model(\n",
    "    user_id,\n",
    "    book_id,\n",
    "    algo,\n",
    "    books_df,\n",
    "    ratings_df,\n",
    "    similarity_matrix,\n",
    "    alpha,\n",
    "    min_rating,\n",
    "    only_CB=False,\n",
    "):\n",
    "    # compute the svd rating\n",
    "    svd_rating = algo.predict(user_id, book_id).est\n",
    "\n",
    "    rating_for_item = ratings_df[\"book_id\"].value_counts().get(book_id, 0)\n",
    "\n",
    "    if rating_for_item < min_rating or only_CB == True:\n",
    "        # compute the content-based rating\n",
    "        user_rated_books = ratings_df[\n",
    "            (ratings_df[\"user\"] == user_id) & (ratings_df[\"book_id\"] != book_id)\n",
    "        ]  # books the user has rated\n",
    "\n",
    "        weighted_sum = 0\n",
    "        similarity_sum = 0\n",
    "\n",
    "        # index of the input book in the similarity matrix\n",
    "        input_index = books_df.index[books_df[\"book_id\"] == book_id].tolist()[0]\n",
    "\n",
    "        # iterate over books the user has rated\n",
    "        for _, rated_item in user_rated_books.iterrows():\n",
    "            rated_book_id = rated_item[\"book_id\"]\n",
    "            rated_book_rating = rated_item[\"rating\"]\n",
    "\n",
    "            # index of the rated book for the similarity matrix\n",
    "            rated_index = books_df.index[books_df[\"book_id\"] == rated_book_id].tolist()[\n",
    "                0\n",
    "            ]\n",
    "\n",
    "            # get cosine similarity between the input book and the rated book\n",
    "            content_similarity = similarity_matrix[input_index][rated_index]\n",
    "\n",
    "            # update weighted sum and similarity sum\n",
    "            weighted_sum += content_similarity * rated_book_rating\n",
    "            similarity_sum += content_similarity\n",
    "\n",
    "        # calculate the content-based predicted rating (weighted average)\n",
    "        if similarity_sum > 0:\n",
    "            cb_rating = weighted_sum / similarity_sum\n",
    "        else:\n",
    "            cb_rating = 0  # No content similarity available\n",
    "\n",
    "        if only_CB:\n",
    "            return cb_rating\n",
    "\n",
    "        if cb_rating != 0:\n",
    "            # combine SVD and CBF predictions\n",
    "            return alpha * svd_rating + (1 - alpha) * cb_rating\n",
    "        else:\n",
    "            return svd_rating\n",
    "    else:\n",
    "        return svd_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to calculate the average embedding for a book\n",
    "def get_book_embedding(text, model):\n",
    "    embeddings = []\n",
    "    if isinstance(model, KeyedVectors): \n",
    "        for word in text:\n",
    "            if word == \"unknown\":\n",
    "                embeddings.append(\n",
    "                    np.zeros(model.vector_size)\n",
    "                )  # if the word is unknown put a neutral vector\n",
    "            elif word in model:  # Check if the word is in the model vocabulary\n",
    "                embeddings.append(model[word])\n",
    "    elif isinstance(model, Word2Vec):\n",
    "        for word in text:\n",
    "            if word == \"unknown\":\n",
    "                embeddings.append(\n",
    "                    np.zeros(model.wv.vector_size)\n",
    "                )  # if the word is unknown, put a neutral vector\n",
    "            elif word in model.wv:  # Check if the word is in the model vocabulary\n",
    "                embeddings.append(model.wv[word])\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)  # Average of all word embeddings\n",
    "    else:\n",
    "        return np.zeros(\n",
    "            model.vector_size\n",
    "        )  # Return a zero vector if no embeddings exist\n",
    "\n",
    "def build_similarity_matrix(dataframe, similarity_measure, model):\n",
    "    df = dataframe.copy()\n",
    "    if model == 'pre-trained': \n",
    "        df[\"book_embedding\"] = df[\"processed_text\"].apply(\n",
    "            lambda x: get_book_embedding(x, pretrained_vectors)\n",
    "        )\n",
    "    elif model == 'local':\n",
    "        df[\"book_embedding\"] = df[\"processed_text\"].apply(\n",
    "            lambda x: get_book_embedding(x, local_model)\n",
    "        )\n",
    "\n",
    "    embedding_matrix = np.vstack(df[\"book_embedding\"].values)\n",
    "    if similarity_measure == 'cosine':\n",
    "        similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "book_val = validation.drop_duplicates(subset=\"book_id\", keep=\"first\")\n",
    "book_val = book_val.drop([\"user\", \"rating\"], axis=1)\n",
    "book_val[\"categories\"] = (\n",
    "    book_val[\"categories\"].fillna(\"unknown\").apply(lambda x: re.sub(r\"[\\[\\]']\", \"\", x))\n",
    ")\n",
    "book_val[\"full_info\"] = book_val[\"categories\"] + \" \" + book_val[\"description\"]\n",
    "book_val[\"processed_text\"] = book_val[\"full_info\"].apply(preprocess_text)\n",
    "book_val = book_val.reset_index()\n",
    "book_val[\"processed_text\"] = book_val[\"processed_text\"].apply(\n",
    "    lambda x: bigram_transformer[x]\n",
    ")\n",
    "ratings_val = validation[[\"user\", \"book_id\", \"rating\"]]\n",
    "\n",
    "# matrix based on pre-trained vectors\n",
    "val_similarity_matrix_1 = build_similarity_matrix(book_val, \"cosine\", model=\"pre-trained\")\n",
    "\n",
    "# matrix based on fine-tuned model\n",
    "val_similarity_matrix_2 = build_similarity_matrix(book_val, \"cosine\", model=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import dump\n",
    "\n",
    "# Load the saved model\n",
    "_, svd_model = dump.load(\"../models/svd_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hybrid(\n",
    "    X, y, param_grid, algo, books_df, ratings_df, only_CB=False\n",
    "):\n",
    "    best_params = None\n",
    "    best_score = float(\"inf\")  \n",
    "    results = [] \n",
    "    # hyper-parameters\n",
    "    alpha_values = param_grid[\"alpha\"]\n",
    "    min_rating_values = param_grid[\"min_rating\"]\n",
    "    similarity_matrix_values = param_grid[\"similarity_matrix\"]\n",
    "\n",
    "    for name, matrix in similarity_matrix_values:\n",
    "        for alpha in alpha_values:\n",
    "            for min_rating in min_rating_values:\n",
    "                predictions = []\n",
    "\n",
    "                for user_id, book_id in X:\n",
    "                    prediction = hybrid_model(\n",
    "                        user_id=user_id,\n",
    "                        book_id=book_id,\n",
    "                        algo=algo,\n",
    "                        books_df=books_df,\n",
    "                        ratings_df=ratings_df,\n",
    "                        similarity_matrix=matrix,\n",
    "                        alpha=alpha,\n",
    "                        min_rating=min_rating,\n",
    "                        only_CB=only_CB,\n",
    "                    )\n",
    "                    predictions.append(prediction)\n",
    "\n",
    "                # RMSE for this combination of parameters\n",
    "                rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"alpha\": alpha,\n",
    "                        \"min_rating\": min_rating,\n",
    "                        \"similarity_matrix\": name,\n",
    "                        \"rmse\": rmse,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # update best parameters if RMSE is lower\n",
    "            if rmse < best_score:\n",
    "                best_score = rmse\n",
    "                best_params = {\n",
    "                    \"alpha\": alpha,\n",
    "                    \"min_rating\": min_rating,\n",
    "                    \"similarity_matrix\": name,\n",
    "                }\n",
    "\n",
    "    return best_params, best_score, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.9, 'min_rating': 10, 'similarity_matrix': 2}\n",
      "Best RMSE: 0.6671597842933592\n",
      "{'alpha': 0.1, 'min_rating': 6, 'similarity_matrix': 1, 'rmse': 0.7660784440917381}\n",
      "{'alpha': 0.1, 'min_rating': 7, 'similarity_matrix': 1, 'rmse': 0.7751021320840445}\n",
      "{'alpha': 0.1, 'min_rating': 8, 'similarity_matrix': 1, 'rmse': 0.7817632792423339}\n",
      "{'alpha': 0.1, 'min_rating': 9, 'similarity_matrix': 1, 'rmse': 0.7873011245379007}\n",
      "{'alpha': 0.1, 'min_rating': 10, 'similarity_matrix': 1, 'rmse': 0.7930643338604606}\n",
      "{'alpha': 0.3, 'min_rating': 6, 'similarity_matrix': 1, 'rmse': 0.7245567615991725}\n",
      "{'alpha': 0.3, 'min_rating': 7, 'similarity_matrix': 1, 'rmse': 0.7303520756207648}\n",
      "{'alpha': 0.3, 'min_rating': 8, 'similarity_matrix': 1, 'rmse': 0.7345427949031311}\n",
      "{'alpha': 0.3, 'min_rating': 9, 'similarity_matrix': 1, 'rmse': 0.7381758284346297}\n",
      "{'alpha': 0.3, 'min_rating': 10, 'similarity_matrix': 1, 'rmse': 0.7419670380268131}\n",
      "{'alpha': 0.5, 'min_rating': 6, 'similarity_matrix': 1, 'rmse': 0.6932809341077372}\n",
      "{'alpha': 0.5, 'min_rating': 7, 'similarity_matrix': 1, 'rmse': 0.6963888914916877}\n",
      "{'alpha': 0.5, 'min_rating': 8, 'similarity_matrix': 1, 'rmse': 0.6985456616182978}\n",
      "{'alpha': 0.5, 'min_rating': 9, 'similarity_matrix': 1, 'rmse': 0.7005584688478989}\n",
      "{'alpha': 0.5, 'min_rating': 10, 'similarity_matrix': 1, 'rmse': 0.7026653934351298}\n",
      "{'alpha': 0.7, 'min_rating': 6, 'similarity_matrix': 1, 'rmse': 0.6736794793940722}\n",
      "{'alpha': 0.7, 'min_rating': 7, 'similarity_matrix': 1, 'rmse': 0.6748431793013975}\n",
      "{'alpha': 0.7, 'min_rating': 8, 'similarity_matrix': 1, 'rmse': 0.6755683505291331}\n",
      "{'alpha': 0.7, 'min_rating': 9, 'similarity_matrix': 1, 'rmse': 0.6763718684767462}\n",
      "{'alpha': 0.7, 'min_rating': 10, 'similarity_matrix': 1, 'rmse': 0.677216174113352}\n",
      "{'alpha': 0.9, 'min_rating': 6, 'similarity_matrix': 1, 'rmse': 0.6667827703661773}\n",
      "{'alpha': 0.9, 'min_rating': 7, 'similarity_matrix': 1, 'rmse': 0.6669195151840658}\n",
      "{'alpha': 0.9, 'min_rating': 8, 'similarity_matrix': 1, 'rmse': 0.6669578563573775}\n",
      "{'alpha': 0.9, 'min_rating': 9, 'similarity_matrix': 1, 'rmse': 0.6670785278074756}\n",
      "{'alpha': 0.9, 'min_rating': 10, 'similarity_matrix': 1, 'rmse': 0.6672063845366163}\n",
      "{'alpha': 0.1, 'min_rating': 6, 'similarity_matrix': 2, 'rmse': 0.7622819502229494}\n",
      "{'alpha': 0.1, 'min_rating': 7, 'similarity_matrix': 2, 'rmse': 0.771459471377352}\n",
      "{'alpha': 0.1, 'min_rating': 8, 'similarity_matrix': 2, 'rmse': 0.7783718252063212}\n",
      "{'alpha': 0.1, 'min_rating': 9, 'similarity_matrix': 2, 'rmse': 0.7838987036053748}\n",
      "{'alpha': 0.1, 'min_rating': 10, 'similarity_matrix': 2, 'rmse': 0.7898025545982267}\n",
      "{'alpha': 0.3, 'min_rating': 6, 'similarity_matrix': 2, 'rmse': 0.7220676389764688}\n",
      "{'alpha': 0.3, 'min_rating': 7, 'similarity_matrix': 2, 'rmse': 0.7279690806662348}\n",
      "{'alpha': 0.3, 'min_rating': 8, 'similarity_matrix': 2, 'rmse': 0.7323482330520477}\n",
      "{'alpha': 0.3, 'min_rating': 9, 'similarity_matrix': 2, 'rmse': 0.7359699894364803}\n",
      "{'alpha': 0.3, 'min_rating': 10, 'similarity_matrix': 2, 'rmse': 0.7398611621969208}\n",
      "{'alpha': 0.5, 'min_rating': 6, 'similarity_matrix': 2, 'rmse': 0.6918944852574371}\n",
      "{'alpha': 0.5, 'min_rating': 7, 'similarity_matrix': 2, 'rmse': 0.6950703087812461}\n",
      "{'alpha': 0.5, 'min_rating': 8, 'similarity_matrix': 2, 'rmse': 0.6973569552968756}\n",
      "{'alpha': 0.5, 'min_rating': 9, 'similarity_matrix': 2, 'rmse': 0.6993616926780679}\n",
      "{'alpha': 0.5, 'min_rating': 10, 'similarity_matrix': 2, 'rmse': 0.701533352337071}\n",
      "{'alpha': 0.7, 'min_rating': 6, 'similarity_matrix': 2, 'rmse': 0.6731141671763482}\n",
      "{'alpha': 0.7, 'min_rating': 7, 'similarity_matrix': 2, 'rmse': 0.674315186843263}\n",
      "{'alpha': 0.7, 'min_rating': 8, 'similarity_matrix': 2, 'rmse': 0.6751155932241799}\n",
      "{'alpha': 0.7, 'min_rating': 9, 'similarity_matrix': 2, 'rmse': 0.675915699638191}\n",
      "{'alpha': 0.7, 'min_rating': 10, 'similarity_matrix': 2, 'rmse': 0.6767950875941822}\n",
      "{'alpha': 0.9, 'min_rating': 6, 'similarity_matrix': 2, 'rmse': 0.6666901718404259}\n",
      "{'alpha': 0.9, 'min_rating': 7, 'similarity_matrix': 2, 'rmse': 0.6668385813528388}\n",
      "{'alpha': 0.9, 'min_rating': 8, 'similarity_matrix': 2, 'rmse': 0.6669010104917972}\n",
      "{'alpha': 0.9, 'min_rating': 9, 'similarity_matrix': 2, 'rmse': 0.6670214335736836}\n",
      "{'alpha': 0.9, 'min_rating': 10, 'similarity_matrix': 2, 'rmse': 0.6671597842933592}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    \"min_rating\": [6, 7, 8, 9, 10],\n",
    "    \"similarity_matrix\": [(1, val_similarity_matrix_1), (2, val_similarity_matrix_2)],\n",
    "}\n",
    "\n",
    "X_val = list(zip(validation[\"user\"], validation[\"book_id\"]))\n",
    "y_val = list(validation[\"rating\"])\n",
    "\n",
    "best_params, best_score, results = grid_search_hybrid(\n",
    "    X=X_val,\n",
    "    y=y_val,\n",
    "    param_grid=param_grid,\n",
    "    algo=svd_model,  \n",
    "    books_df=book_val[[\"book_id\"]],\n",
    "    ratings_df=ratings_val,\n",
    "    only_CB=False,\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best RMSE:\", best_score)\n",
    "\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hybrid(X, y, param_grid, algo, books_df, ratings_df, matrix, only_CB=False):\n",
    "    best_params = None\n",
    "    best_score = float(\"inf\")\n",
    "    results = []\n",
    "    # hyper-parameters\n",
    "    alpha_values = param_grid[\"alpha\"]\n",
    "    min_rating_values = param_grid[\"min_rating\"]\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        for min_rating in min_rating_values:\n",
    "            predictions = []\n",
    "            for user_id, book_id in X:\n",
    "             prediction = hybrid_model(\n",
    "                        user_id=user_id,\n",
    "                        book_id=book_id,\n",
    "                        algo=algo,\n",
    "                        books_df=books_df,\n",
    "                        ratings_df=ratings_df,\n",
    "                        similarity_matrix=matrix,\n",
    "                        alpha=alpha,\n",
    "                        min_rating=min_rating,\n",
    "                        only_CB=only_CB,\n",
    "                    )\n",
    "             predictions.append(prediction)\n",
    "\n",
    "            # RMSE for this combination of parameters\n",
    "        rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "        results.append(\n",
    "                    {\n",
    "                        \"alpha\": alpha,\n",
    "                        \"min_rating\": min_rating,\n",
    "                        \"rmse\": rmse,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # update best parameters if RMSE is lower\n",
    "        if rmse < best_score:\n",
    "                best_score = rmse\n",
    "                best_params = {\n",
    "                    \"alpha\": alpha,\n",
    "                    \"min_rating\": min_rating,\n",
    "\n",
    "                }\n",
    "\n",
    "    return best_params, best_score, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test best parameters on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "book_test = test.drop_duplicates(subset=\"book_id\", keep=\"first\")\n",
    "book_test = book_test.drop([\"user\", \"rating\"], axis=1)\n",
    "book_test[\"categories\"] = (\n",
    "    book_test[\"categories\"].fillna(\"unknown\").apply(lambda x: re.sub(r\"[\\[\\]']\", \"\", x))\n",
    ")\n",
    "book_test[\"full_info\"] = book_test[\"categories\"] + \" \" + book_test[\"description\"]\n",
    "book_test[\"processed_text\"] = book_test[\"full_info\"].apply(preprocess_text)\n",
    "book_test = book_test.reset_index()\n",
    "book_test[\"processed_text\"] = book_test[\"processed_text\"].apply(lambda x: bigram_transformer[x])\n",
    "test_similarity_matrix_2 = build_similarity_matrix(book_test, \"cosine\", \"local\")\n",
    "ratings_test = test[[\"user\", \"book_id\", \"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4407001753364538 0.6638525252919159\n"
     ]
    }
   ],
   "source": [
    "X_test = list(zip(test[\"user\"], test[\"book_id\"]))\n",
    "y_test = list(test[\"rating\"])\n",
    "\n",
    "predictions = []\n",
    "for user_id, book_id in X_test:\n",
    "    prediction = hybrid_model(\n",
    "        user_id=user_id,\n",
    "        book_id=book_id,\n",
    "        algo=svd_model,\n",
    "        books_df=book_test,\n",
    "        ratings_df=ratings_test,\n",
    "        similarity_matrix=test_similarity_matrix_2,\n",
    "        alpha=0.9,\n",
    "        min_rating=10,\n",
    "        only_CB=False,\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "\n",
    "mae = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mae)\n",
    "print(mae, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test only the content-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7756023918681647 1.9430909376218513\n"
     ]
    }
   ],
   "source": [
    "X_test = list(zip(test[\"user\"], test[\"book_id\"]))\n",
    "y_test = list(test[\"rating\"])\n",
    "\n",
    "predictions = []\n",
    "for user_id, book_id in X_test:\n",
    "    prediction = hybrid_model(\n",
    "        user_id=user_id,\n",
    "        book_id=book_id,\n",
    "        algo=svd_model,\n",
    "        books_df=book_test,\n",
    "        ratings_df=ratings_test,\n",
    "        similarity_matrix=test_similarity_matrix_2,\n",
    "        alpha=0.9,\n",
    "        min_rating=8,\n",
    "        only_CB=True,\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "\n",
    "mae = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mae)\n",
    "print(mae, rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-github",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
